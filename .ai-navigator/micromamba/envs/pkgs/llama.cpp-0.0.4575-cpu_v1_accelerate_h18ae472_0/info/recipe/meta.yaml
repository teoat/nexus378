# This file created by conda-build 24.5.1
# ------------------------------------------------

package:
  name: llama.cpp
  version: 0.0.4575
source:
  git_rev: b4575
  git_url: https://github.com/ggerganov/llama.cpp.git
  patches:
    - patches/fix-convert_lora_to_gguf.patch
    - patches/loosen-max_nmse_err.patch
    - patches/metal_gpu_selection.patch
build:
  number: 0
  string: cpu_v1_accelerate_h18ae472_0
requirements:
  build:
    - bzip2 1.0.8 h6c40b1e_6
    - c-ares 1.19.1 h6c40b1e_0
    - ca-certificates 2024.12.31 hecd8cb5_0
    - cctools_osx-64 949.0.1 hc7db93f_25
    - clang 14.0.6 hecd8cb5_2
    - clang-14 14.0.6 default_h182e8cd_2
    - clang_osx-64 14.0.6 hb1e4b1b_0
    - clangxx 14.0.6 default_h182e8cd_2
    - clangxx_osx-64 14.0.6 hd8b9576_0
    - cmake 3.31.2 hc67f837_0
    - compiler-rt 14.0.6 hda8b6b8_0
    - compiler-rt_osx-64 14.0.6 h8d5cb93_0
    - curl 8.11.1 h184c1cd_0
    - expat 2.6.4 h6d0c2b6_0
    - gettext 0.21.0 h4e8c18a_2
    - git 2.45.2 pl5340h6ecf5f8_1
    - icu 73.1 hcec6c5f_0
    - krb5 1.20.1 h428f121_1
    - ld64_osx-64 530 h70f3046_25
    - ldid 2.1.5 hc58f1be_3
    - libclang-cpp14 14.0.6 default_h182e8cd_2
    - libcurl 8.11.1 h9bcc28a_0
    - libcxx 14.0.6 h9765a3e_0
    - libedit 3.1.20230828 h6c40b1e_0
    - libev 4.33 h9ed2024_1
    - libffi 3.4.4 hecd8cb5_1
    - libiconv 1.16 h6c40b1e_3
    - libllvm14 14.0.6 h26321d7_4
    - libmpdec 4.0.0 h46256e1_0
    - libnghttp2 1.57.0 h9beae6a_0
    - libssh2 1.11.1 h3a17b82_0
    - libuv 1.48.0 h46256e1_0
    - libxml2 2.13.5 h6070cd6_0
    - llvm-openmp 14.0.6 h0dcd299_0
    - llvm-tools 14.0.6 h91fad77_4
    - lz4-c 1.9.4 hcec6c5f_1
    - ncurses 6.4 hcec6c5f_0
    - ninja-base 1.12.1 h1962661_0
    - openssl 3.0.15 h46256e1_0
    - patch 2.7.6 h1de35cc_1001
    - pcre2 10.42 h9b97e30_1
    - perl 5.38.2 0_h46256e1_perl5
    - pkg-config 0.29.2 h3efe00b_8
    - pkgconfig 1.5.5 py313hecd8cb5_0
    - python 3.13.1 h1ec50cd_100_cp313
    - python_abi 3.13 0_cp313
    - readline 8.2 hca72f7f_0
    - rhash 1.4.3 h04015c4_0
    - sqlite 3.45.3 h6c40b1e_0
    - tapi 1000.10.8 ha1b3eb9_0
    - tk 8.6.14 h4d00af3_0
    - tzdata 2025a h04d1e81_0
    - xz 5.4.6 h6c40b1e_1
    - zlib 1.2.13 h4b97444_1
    - zstd 1.5.6 h138b38a_0
  host:
    - libcxx 14.0.6 h9765a3e_0
    - llvm-openmp 14.0.6 h0dcd299_0
  run:
    - __osx >=10.15
    - libcxx >=14.0.6
    - llvm-openmp >=14.0.6
test:
  commands:
    - llama-cli --help   || true
    - llama-server --help || true
    - test -f $PREFIX/include/llama.h
    - test -f $PREFIX/bin/llama-cli
    - test -f $PREFIX/bin/llama-server
    - test -f $PREFIX/lib/libllama.dylib
about:
  description: 'Inference of Meta''s LLaMA model (and others) in pure C/C++

    '
  dev_url: https://github.com/ggerganov/llama.cpp
  doc_url: https://github.com/ggerganov/llama.cpp
  home: https://github.com/ggerganov/llama.cpp
  license: MIT
  license_family: MIT
  license_file: LICENSE
  summary: LLM inference in C/C++
extra:
  copy_test_source_files: true
  final: true
  flow_run_id: 40574d7f-73bc-41b0-a680-e3f5dc61d7f7
  remote_url: git@github.com:AnacondaRecipes/llama.cpp-feedstock.git
  sha: dc9e90da5d71e7cba26f14eb5067ddadc6aaa23b
