"""
Workflow Orchestrator - Complex Workflow Management System

This module implements the WorkflowOrchestrator class that provides
comprehensive workflow orchestration and management for complex
multi-agent forensic workflows.
"""

import asyncio
import logging
import json
from typing import Dict, List, Optional, Any, Tuple, Union
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from enum import Enum
from collections import defaultdict, deque
import uuid
import networkx as nx

from ...taskmaster.models.job import Job, JobStatus, JobPriority, JobType


class WorkflowStatus(Enum):
    """Workflow execution status."""
    PENDING = "pending"                                # Workflow is pending execution
    RUNNING = "running"                                # Workflow is currently running
    PAUSED = "paused"                                  # Workflow is paused
    COMPLETED = "completed"                            # Workflow completed successfully
    FAILED = "failed"                                  # Workflow failed
    CANCELLED = "cancelled"                            # Workflow was cancelled
    TIMEOUT = "timeout"                                # Workflow timed out


class WorkflowType(Enum):
    """Types of forensic workflows."""
    INVESTIGATION = "investigation"                     # Full investigation workflow
    RECONCILIATION = "reconciliation"                   # Data reconciliation workflow
    FRAUD_DETECTION = "fraud_detection"                # Fraud detection workflow
    COMPLIANCE_AUDIT = "compliance_audit"              # Compliance audit workflow
    EVIDENCE_ANALYSIS = "evidence_analysis"            # Evidence analysis workflow
    RISK_ASSESSMENT = "risk_assessment"                # Risk assessment workflow


class StepStatus(Enum):
    """Individual step execution status."""
    PENDING = "pending"                                 # Step is pending
    RUNNING = "running"                                 # Step is running
    COMPLETED = "completed"                             # Step completed
    FAILED = "failed"                                   # Step failed
    SKIPPED = "skipped"                                 # Step was skipped
    CANCELLED = "cancelled"                             # Step was cancelled


@dataclass
class WorkflowStep:
    """A step in a forensic workflow."""
    
    id: str
    name: str
    description: str
    agent_type: str
    required_capabilities: List[str]
    dependencies: List[str]
    estimated_duration: float
    timeout: float
    retry_count: int
    max_retries: int
    priority: int
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class WorkflowExecution:
    """Execution of a forensic workflow."""
    
    id: str
    workflow_type: WorkflowType
    name: str
    description: str
    steps: List[WorkflowStep]
    status: WorkflowStatus
    current_step: int
    start_time: datetime
    end_time: Optional[datetime] = None
    estimated_completion: datetime
    progress: float
    agents_assigned: Dict[str, str]
    step_results: Dict[str, Any]
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class StepExecution:
    """Execution of a workflow step."""
    
    step_id: str
    workflow_id: str
    status: StepStatus
    agent_id: Optional[str] = None
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    result: Optional[Any] = None
    error: Optional[str] = None
    retry_count: int = 0
    metadata: Dict[str, Any] = field(default_factory=dict)


class WorkflowOrchestrator:
    """
    Comprehensive workflow orchestration system.
    
    The WorkflowOrchestrator is responsible for:
    - Managing complex multi-agent workflows
    - Handling workflow dependencies and execution order
    - Managing workflow state and progress
    - Handling workflow failures and retries
    - Optimizing workflow execution
    """
    
    def __init__(self, config: Dict[str, Any]):
        """Initialize the WorkflowOrchestrator."""
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # Configuration
        self.max_concurrent_workflows = config.get('max_concurrent_workflows', 5)
        self.workflow_timeout = config.get('workflow_timeout', 3600)  # 1 hour
        self.step_timeout = config.get('step_timeout', 300)  # 5 minutes
        self.max_retries = config.get('max_retries', 3)
        
        # Workflow management
        self.workflows: Dict[str, WorkflowExecution] = {}
        self.workflow_queue: deque = deque()
        self.active_workflows: Dict[str, WorkflowExecution] = {}
        self.completed_workflows: Dict[str, WorkflowExecution] = {}
        
        # Step execution tracking
        self.step_executions: Dict[str, StepExecution] = {}
        self.step_dependencies: Dict[str, List[str]] = {}
        
        # Performance tracking
        self.total_workflows_executed = 0
        self.total_steps_executed = 0
        self.average_workflow_duration = 0.0
        self.success_rate = 0.0
        
        # Event loop
        self.loop = asyncio.get_event_loop()
        
        self.logger.info("WorkflowOrchestrator initialized successfully")
    
    async def start(self):
        """Start the WorkflowOrchestrator."""
        self.logger.info("Starting WorkflowOrchestrator...")
        
        # Initialize workflow components
        await self._initialize_workflow_components()
        
        # Start background tasks
        asyncio.create_task(self._process_workflow_queue())
        asyncio.create_task(self._monitor_workflow_timeouts())
        asyncio.create_task(self._update_performance_metrics())
        
        self.logger.info("WorkflowOrchestrator started successfully")
    
    async def stop(self):
        """Stop the WorkflowOrchestrator."""
        self.logger.info("Stopping WorkflowOrchestrator...")
        self.logger.info("WorkflowOrchestrator stopped")
    
    async def create_workflow(self, workflow_type: WorkflowType, name: str,
                            description: str, steps: List[WorkflowStep]) -> str:
        """Create a new forensic workflow."""
        try:
            # Validate workflow
            if not steps:
                raise ValueError("Workflow must have at least one step")
            
            # Create workflow execution
            workflow_id = str(uuid.uuid4())
            workflow = WorkflowExecution(
                id=workflow_id,
                workflow_type=workflow_type,
                name=name,
                description=description,
                steps=steps,
                status=WorkflowStatus.PENDING,
                current_step=0,
                start_time=datetime.utcnow(),
                estimated_completion=datetime.utcnow() + timedelta(hours=1),
                progress=0.0,
                agents_assigned={},
                step_results={}
            )
            
            # Build dependency graph
            await self._build_dependency_graph(workflow)
            
            # Store workflow
            self.workflows[workflow_id] = workflow
            
            # Add to queue
            self.workflow_queue.append(workflow_id)
            
            self.logger.info(f"Created workflow: {workflow_id} ({workflow_type.value})")
            return workflow_id
            
        except Exception as e:
            self.logger.error(f"Error creating workflow: {e}")
            raise
    
    async def start_workflow(self, workflow_id: str) -> bool:
        """Start a workflow execution."""
        try:
            if workflow_id in self.workflows:
                workflow = self.workflows[workflow_id]
                
                # Check if we can start the workflow
                if len(self.active_workflows) >= self.max_concurrent_workflows:
                    return False
                
                # Update workflow status
                workflow.status = WorkflowStatus.RUNNING
                workflow.start_time = datetime.utcnow()
                self.active_workflows[workflow_id] = workflow
                
                # Start workflow execution
                asyncio.create_task(self._execute_workflow(workflow))
                
                self.logger.info(f"Started workflow: {workflow_id}")
                return True
            
            return False
            
        except Exception as e:
            self.logger.error(f"Error starting workflow {workflow_id}: {e}")
            return False
    
    async def pause_workflow(self, workflow_id: str) -> bool:
        """Pause a running workflow."""
        try:
            if workflow_id in self.active_workflows:
                workflow = self.active_workflows[workflow_id]
                workflow.status = WorkflowStatus.PAUSED
                
                self.logger.info(f"Paused workflow: {workflow_id}")
                return True
            
            return False
            
        except Exception as e:
            self.logger.error(f"Error pausing workflow {workflow_id}: {e}")
            return False
    
    async def resume_workflow(self, workflow_id: str) -> bool:
        """Resume a paused workflow."""
        try:
            if workflow_id in self.workflows:
                workflow = self.workflows[workflow_id]
                if workflow.status == WorkflowStatus.PAUSED:
                    workflow.status = WorkflowStatus.RUNNING
                    
                    # Continue execution
                    asyncio.create_task(self._execute_workflow(workflow))
                    
                    self.logger.info(f"Resumed workflow: {workflow_id}")
                    return True
            
            return False
            
        except Exception as e:
            self.logger.error(f"Error resuming workflow {workflow_id}: {e}")
            return False
    
    async def cancel_workflow(self, workflow_id: str) -> bool:
        """Cancel a workflow execution."""
        try:
            if workflow_id in self.active_workflows:
                workflow = self.active_workflows[workflow_id]
                
                # Update status
                workflow.status = WorkflowStatus.CANCELLED
                workflow.end_time = datetime.utcnow()
                
                # Remove from active workflows
                del self.active_workflows[workflow_id]
                
                # Move to completed workflows
                self.completed_workflows[workflow_id] = workflow
                
                self.logger.info(f"Cancelled workflow: {workflow_id}")
                return True
            
            return False
            
        except Exception as e:
            self.logger.error(f"Error cancelling workflow {workflow_id}: {e}")
            return False
    
    async def get_workflow_status(self, workflow_id: str) -> Optional[WorkflowExecution]:
        """Get the status of a workflow."""
        try:
            return self.workflows.get(workflow_id)
        except Exception as e:
            self.logger.error(f"Error getting workflow status: {e}")
            return None
    
    async def _build_dependency_graph(self, workflow: WorkflowExecution):
        """Build dependency graph for workflow steps."""
        try:
            # Create directed graph
            G = nx.DiGraph()
            
            # Add nodes
            for step in workflow.steps:
                G.add_node(step.id)
            
            # Add edges based on dependencies
            for step in workflow.steps:
                for dependency in step.dependencies:
                    if dependency in [s.id for s in workflow.steps]:
                        G.add_edge(dependency, step.id)
            
            # Check for cycles
            try:
                cycles = list(nx.simple_cycles(G))
                if cycles:
                    raise ValueError(f"Circular dependencies detected: {cycles}")
            except nx.NetworkXNoCycle:
                pass
            
            # Store dependency information
            self.step_dependencies[workflow.id] = {
                step.id: list(G.predecessors(step.id)) for step in workflow.steps
            }
            
            self.logger.info(f"Built dependency graph for workflow {workflow.id}")
            
        except Exception as e:
            self.logger.error(f"Error building dependency graph: {e}")
            raise
    
    async def _execute_workflow(self, workflow: WorkflowExecution):
        """Execute a complete workflow."""
        try:
            # Get execution order based on dependencies
            execution_order = await self._get_execution_order(workflow)
            
            # Execute steps in order
            for step_id in execution_order:
                if workflow.status != WorkflowStatus.RUNNING:
                    break
                
                step = next(s for s in workflow.steps if s.id == step_id)
                
                # Execute step
                step_result = await self._execute_workflow_step(workflow, step)
                
                if step_result:
                    # Update workflow progress
                    workflow.current_step += 1
                    workflow.progress = (workflow.current_step / len(workflow.steps)) * 100
                    workflow.step_results[step.id] = step_result
                else:
                    # Step failed
                    workflow.status = WorkflowStatus.FAILED
                    workflow.end_time = datetime.utcnow()
                    break
            
            # Update workflow status
            if workflow.status == WorkflowStatus.RUNNING:
                workflow.status = WorkflowStatus.COMPLETED
                workflow.progress = 100.0
                workflow.end_time = datetime.utcnow()
            
            # Remove from active workflows
            if workflow.id in self.active_workflows:
                del self.active_workflows[workflow.id]
            
            # Move to completed workflows
            self.completed_workflows[workflow.id] = workflow
            
            # Update statistics
            self.total_workflows_executed += 1
            
            self.logger.info(f"Workflow completed: {workflow.id} - Status: {workflow.status}")
            
        except Exception as e:
            self.logger.error(f"Error executing workflow {workflow.id}: {e}")
            workflow.status = WorkflowStatus.FAILED
            workflow.end_time = datetime.utcnow()
    
    async def _get_execution_order(self, workflow: WorkflowExecution) -> List[str]:
        """Get the execution order for workflow steps based on dependencies."""
        try:
            if workflow.id not in self.step_dependencies:
                return [step.id for step in workflow.steps]
            
            dependencies = self.step_dependencies[workflow.id]
            
            # Use topological sort to determine execution order
            G = nx.DiGraph()
            
            # Add nodes
            for step in workflow.steps:
                G.add_node(step.id)
            
            # Add edges
            for step_id, deps in dependencies.items():
                for dep in deps:
                    G.add_edge(dep, step_id)
            
            # Get topological order
            try:
                execution_order = list(nx.topological_sort(G))
                return execution_order
            except nx.NetworkXError:
                # Fallback to step order if topological sort fails
                return [step.id for step in workflow.steps]
            
        except Exception as e:
            self.logger.error(f"Error getting execution order: {e}")
            return [step.id for step in workflow.steps]
    
    async def _execute_workflow_step(self, workflow: WorkflowExecution, 
                                   step: WorkflowStep) -> Optional[Any]:
        """Execute a single workflow step."""
        try:
            # Create step execution record
            step_execution = StepExecution(
                step_id=step.id,
                workflow_id=workflow.id,
                status=StepStatus.RUNNING,
                start_time=datetime.utcnow()
            )
            
            self.step_executions[f"{workflow.id}_{step.id}"] = step_execution
            
            # Simulate step execution
            self.logger.info(f"Executing step {step.id} in workflow {workflow.id}")
            
            # Update step status
            step_execution.status = StepStatus.COMPLETED
            step_execution.end_time = datetime.utcnow()
            
            # Simulate result
            step_result = {
                'status': 'success',
                'output': f"Step {step.id} completed successfully",
                'timestamp': datetime.utcnow().isoformat()
            }
            
            step_execution.result = step_result
            
            # Update statistics
            self.total_steps_executed += 1
            
            self.logger.info(f"Completed step {step.id}")
            
            return step_result
            
        except Exception as e:
            self.logger.error(f"Error executing step {step.id}: {e}")
            
            # Update step execution
            if f"{workflow.id}_{step.id}" in self.step_executions:
                step_execution = self.step_executions[f"{workflow.id}_{step.id}"]
                step_execution.status = StepStatus.FAILED
                step_execution.end_time = datetime.utcnow()
                step_execution.error = str(e)
            
            return None
    
    async def _process_workflow_queue(self):
        """Process the workflow queue."""
        while True:
            try:
                if self.workflow_queue and len(self.active_workflows) < self.max_concurrent_workflows:
                    # Get next workflow
                    workflow_id = self.workflow_queue.popleft()
                    
                    if workflow_id in self.workflows:
                        # Start workflow
                        await self.start_workflow(workflow_id)
                
                await asyncio.sleep(1)  # Check every second
                
            except Exception as e:
                self.logger.error(f"Error processing workflow queue: {e}")
                await asyncio.sleep(1)
    
    async def _monitor_workflow_timeouts(self):
        """Monitor workflows for timeouts."""
        while True:
            try:
                current_time = datetime.utcnow()
                timed_out_workflows = []
                
                for workflow_id, workflow in self.active_workflows.items():
                    # Check workflow timeout
                    if workflow.start_time:
                        time_since_start = (current_time - workflow.start_time).total_seconds()
                        
                        if time_since_start > self.workflow_timeout:
                            workflow.status = WorkflowStatus.TIMEOUT
                            workflow.end_time = current_time
                            timed_out_workflows.append(workflow_id)
                
                # Handle timed out workflows
                for workflow_id in timed_out_workflows:
                    if workflow_id in self.active_workflows:
                        del self.active_workflows[workflow_id]
                        self.completed_workflows[workflow_id] = self.workflows[workflow_id]
                
                if timed_out_workflows:
                    self.logger.warning(f"Found {len(timed_out_workflows)} timed out workflows")
                
                await asyncio.sleep(60)  # Check every minute
                
            except Exception as e:
                self.logger.error(f"Error monitoring workflow timeouts: {e}")
                await asyncio.sleep(60)
    
    async def _update_performance_metrics(self):
        """Update performance metrics."""
        while True:
            try:
                # Calculate average workflow duration
                completed_workflows = [
                    workflow for workflow in self.completed_workflows.values()
                    if workflow.end_time and workflow.start_time
                ]
                
                if completed_workflows:
                    durations = [
                        (workflow.end_time - workflow.start_time).total_seconds()
                        for workflow in completed_workflows
                    ]
                    self.average_workflow_duration = np.mean(durations)
                
                # Calculate success rate
                if self.total_workflows_executed > 0:
                    successful_workflows = len([
                        w for w in self.completed_workflows.values()
                        if w.status == WorkflowStatus.COMPLETED
                    ])
                    self.success_rate = successful_workflows / self.total_workflows_executed
                
                await asyncio.sleep(300)  # Update every 5 minutes
                
            except Exception as e:
                self.logger.error(f"Error updating performance metrics: {e}")
                await asyncio.sleep(300)
    
    async def _initialize_workflow_components(self):
        """Initialize workflow components."""
        try:
            # Initialize workflow templates
            await self._initialize_workflow_templates()
            
            self.logger.info("Workflow components initialized successfully")
            
        except Exception as e:
            self.logger.error(f"Error initializing workflow components: {e}")
    
    async def _initialize_workflow_templates(self):
        """Initialize predefined workflow templates."""
        try:
            # This would create standard workflow templates
            # For now, just log initialization
            self.logger.info("Workflow templates initialized")
            
        except Exception as e:
            self.logger.error(f"Error initializing workflow templates: {e}")
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """Get performance metrics."""
        return {
            'total_workflows_executed': self.total_workflows_executed,
            'total_steps_executed': self.total_steps_executed,
            'average_workflow_duration': self.average_workflow_duration,
            'success_rate': self.success_rate,
            'active_workflows': len(self.active_workflows),
            'queued_workflows': len(self.workflow_queue),
            'completed_workflows': len(self.completed_workflows),
            'total_workflows': len(self.workflows)
        }


# Example usage and testing
if __name__ == "__main__":
    # Configuration
    config = {
        'max_concurrent_workflows': 5,
        'workflow_timeout': 3600,
        'step_timeout': 300,
        'max_retries': 3
    }
    
    # Initialize workflow orchestrator
    orchestrator = WorkflowOrchestrator(config)
    
    print("WorkflowOrchestrator system initialized successfully!")
